# -*- coding: utf-8 -*-
"""credit_card_fraud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KR0uaZrb-Z8W_jT2Bb1-S7W9Fp2YZZ6s

# **CREDIT CARD FRAUD DETECTION**

### Import the required libraries
"""

from matplotlib import pyplot as plt
import seaborn as sns
from IPython.display import Image

import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
from matplotlib import pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from collections import Counter

df = pd.read_csv("/content/creditcard.csv")

df.shape

df.head

"""### Going through the dataset"""

labels=["Genuine","Fraud"]

fraud_or_not = df["Class"].value_counts().tolist()
values = [fraud_or_not[0], fraud_or_not[1]]

fig = px.pie(values=df['Class'].value_counts(), names=labels , width=700, height=400, color_discrete_sequence=["skyblue","black"]
             ,title="Fraud vs Genuine transactions")
fig.show()

sns.countplot(x='Class',data=df,palette="twilight")

print('Genuine:', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')
print('Frauds:', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')

"""## **DATA PREPROCESSING**

### Removing Duplicate and Null Values
"""

df.drop_duplicates(inplace=True)

df.dropna(axis=0,inplace=True)

df = df.drop('Time', axis=1)

"""### **EXPLORATORY DATA ANALYSIS**

### Dealing with Outliers
"""

numeric_columns = (list(df.loc[:, 'V1':'Amount']))

# checking boxplots
def boxplots_custom(dataset, columns_list, rows, cols, suptitle):
    fig, axs = plt.subplots(rows, cols, sharey=True, figsize=(16,25))
    fig.suptitle(suptitle,y=1, size=25)
    axs = axs.flatten()
    for i, data in enumerate(columns_list):
        sns.boxplot(data=dataset[data], orient='h', ax=axs[i])
        axs[i].set_title(data + ', skewness is: '+str(round(dataset[data].skew(axis = 0, skipna = True),2)))

boxplots_custom(dataset=df, columns_list=numeric_columns, rows=10, cols=3, suptitle='Boxplots for each variable')
plt.tight_layout()

def IQR_method (df,n,features):
    """
    Takes a dataframe and returns an index list corresponding to the observations
    containing more than n outliers according to the Tukey IQR method.
    """
    outlier_list = []

    for column in features:
      q1=df[column].quantile(0.25)
      q3=df[column].quantile(0.75)
      IQR=q3-q1
      upper=q3+1.5*IQR
      lower=q1-1.5*IQR
      outlier_list_column = df[(df[column] < lower) | (df[column] > upper )].index
      outlier_list.extend(outlier_list_column)

    outlier_list = Counter(outlier_list)
    multiple_outliers = list( k for k, v in outlier_list.items() if v > n )

    # Calculate the number of records below and above lower and above bound value respectively
    out1 = df[df[column] < q1 - 1.5*IQR]
    out2 = df[df[column] > q3 + 1.5*IQR]

    print('Total number of deleted outliers is:', out1.shape[0]+out2.shape[0])

    return multiple_outliers

# detecting outliers
Outliers = IQR_method(df,1,numeric_columns)

df_out = df.drop(Outliers, axis = 0).reset_index(drop=True)

df.shape

sns.countplot(x='Class',data=df,palette="twilight")

"""### Stratified splitting"""

x = df.drop('Class', axis=1)
y = df['Class']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size = 0.3, random_state = 42)

"""### Feature scaling"""

from sklearn.preprocessing import StandardScaler

# Creating function for scaling
def Standard_Scaler (df, col_names):
    features = df[col_names]
    scaler = StandardScaler().fit(features.values)
    features = scaler.transform(features.values)
    df[col_names] = features

    return df

col_names = ['Amount']
x_train = Standard_Scaler (x_train, col_names)
x_test = Standard_Scaler (x_test, col_names)

"""## **MODEL SELECTION AND EVALUATION**"""

from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

kf = StratifiedKFold(n_splits=5, shuffle=False)

rf = RandomForestClassifier(n_estimators=100, random_state=13)

score = cross_val_score(rf, x_train, y_train, cv=kf, scoring='recall')
print("Cross Validation Recall scores are: {}".format(score))
print("Average Cross Validation Recall score: {}".format(score.mean()))

from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [4, 6, 10, 12],
    'random_state': [13]
}

grid_rf = GridSearchCV(rf, param_grid=params, cv=kf,
                          scoring='recall').fit(x_train, y_train)

print('Best parameters:', grid_rf.best_params_)
print('Best score:', grid_rf.best_score_)

y_pred = grid_rf.predict(x_test)

from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred))

